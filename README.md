<p align="center">
  <h1 align="center">ğŸ™ï¸ VoiceBench</h1>
  <p align="center">
    Open-source voice AI evaluation workbench for dev teams
  </p>
</p>

<p align="center">
  <a href="https://github.com/mhmdez/voicebench/blob/main/LICENSE"><img alt="MIT License" src="https://img.shields.io/badge/license-MIT-blue.svg" /></a>
  <a href="https://nextjs.org"><img alt="Next.js 16" src="https://img.shields.io/badge/Next.js-16-black?logo=next.js" /></a>
  <a href="https://www.typescriptlang.org"><img alt="TypeScript" src="https://img.shields.io/badge/TypeScript-5-3178C6?logo=typescript&logoColor=white" /></a>
  <a href="https://tailwindcss.com"><img alt="Tailwind CSS" src="https://img.shields.io/badge/Tailwind_CSS-4-38B2AC?logo=tailwind-css&logoColor=white" /></a>
  <a href="https://orm.drizzle.team"><img alt="Drizzle ORM" src="https://img.shields.io/badge/Drizzle-ORM-C5F74F?logo=drizzle&logoColor=black" /></a>
</p>

<p align="center">
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-features">Features</a> â€¢
  <a href="#-screenshots">Screenshots</a> â€¢
  <a href="#-architecture">Architecture</a> â€¢
  <a href="#-providers">Providers</a> â€¢
  <a href="#-contributing">Contributing</a>
</p>

---

## What is VoiceBench?

**VoiceBench** is an open-source evaluation workbench for teams building voice AI applications. Hook up your voice providers, run real conversations against evaluation prompts, and get comprehensive quality metrics â€” both auto-detected and human-rated â€” in one place.

If you're building a voice agent and your current eval process is "listen to it and see if it sounds okay," this is for you.

### The Problem

Voice AI teams lack standardized tooling to evaluate their agents. Text LLMs have dozens of eval frameworks. Voice agents? You're on your own. You can't easily answer:

- How fast does my agent respond? (TTFB, latency)
- How accurate is the speech recognition? (WER)
- Does it sound natural? Handle interruptions well? (Human judgment)
- How does Provider A compare to Provider B on the same prompts?
- Are we getting better or worse over time?

### The Solution

VoiceBench gives you a structured workflow: **pick a provider â†’ pick a prompt â†’ have a conversation â†’ rate the responses â†’ analyze across sessions.** Auto-detected metrics (latency, WER, speech rate) are captured automatically. Human quality metrics (naturalness, emotion, turn-taking) are one-click ratings per turn. Everything feeds into a cross-session analytics dashboard you can filter by provider, prompt, or metric.

## ğŸ“¸ Screenshots

### Live Eval â€” Conversation with Real-Time Metrics
Start a session with any configured provider. Auto metrics update live. Rate each turn on 8 quality dimensions with one click.

![Live Eval](docs/screenshots/eval.png)

### Results â€” Cross-Session Analytics
Compare providers, analyze by prompt, break down by metric. Export to CSV.

![Results](docs/screenshots/results.png)

### Prompts â€” 75+ Evaluation Scenarios
Built-in scenarios across task completion, information retrieval, and conversation flow. Create your own or import from YAML.

![Prompts](docs/screenshots/prompts.png)

### Settings â€” Provider Configuration
Add and manage voice AI providers. Test connections before running evals.

![Settings](docs/screenshots/settings.png)

## ğŸš€ Quick Start

### Prerequisites

- **Node.js 20+**
- **npm** or **pnpm**
- SQLite (bundled via better-sqlite3)

### Installation

```bash
git clone https://github.com/mhmdez/voicebench.git
cd voicebench

npm install

cp .env.example .env.local
# Add your provider API keys to .env.local

npm run db:push

npm run dev
```

Open [http://localhost:3000](http://localhost:3000) â€” you'll land on the Live Eval page.

### Environment Variables

```env
# Database (SQLite, local by default)
DATABASE_URL=./data/voicebench.db

# Provider keys (add via Settings UI or env)
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...
RETELL_API_KEY=...

# Judge model for automated scoring (optional)
JUDGE_MODEL=gpt-4o
JUDGE_API_KEY=sk-...

# Whisper for transcription/WER (optional)
WHISPER_API_KEY=sk-...
```

## âœ¨ Features

### ğŸ¯ Live Eval

The core workflow:

1. **Choose provider + prompt** â€” Select from configured providers and 75+ built-in scenarios, or type a freestyle prompt
2. **Converse** â€” Multi-turn conversation with the voice agent
3. **Rate per turn** â€” Quick thumbs up/down on 8 quality dimensions
4. **Watch metrics live** â€” Auto-detected metrics and sparkline trends update after each turn
5. **End & save** â€” Session saved with full metrics for cross-session analysis

### ğŸ“Š Auto-Detected Metrics

Measured automatically during every conversation â€” no manual work required:

| Metric | What it measures |
|--------|-----------------|
| **TTFB** | Time to first byte â€” how fast the agent starts responding |
| **Response Time** | Total end-to-end response latency |
| **Word Count** | Response verbosity per turn |
| **Speech Rate** | Words per minute â€” pacing analysis |
| **WER** | Word Error Rate â€” transcription accuracy vs expected |
| **Audio Duration** | Length of audio responses in seconds |

Real-time sparkline charts show trends across turns so you can spot degradation mid-conversation.

### ğŸ‘¤ Human Rating Metrics

Some things only a human can judge. One click per turn, per metric:

| Metric | What it captures |
|--------|-----------------|
| **Naturalness** | Does it sound like a real person talking? |
| **Prosody** | Rhythm, stress, intonation â€” the musicality of speech |
| **Emotion** | Appropriate emotional expression for the context |
| **Accuracy** | Factual correctness of the agent's responses |
| **Helpfulness** | Did it actually help accomplish the task? |
| **Efficiency** | Got to the point without unnecessary rambling? |
| **Turn-taking** | Natural conversational flow and response timing |
| **Interruption Handling** | Graceful handling when the user interrupts |

Three-state rating: ğŸ‘ positive, ğŸ‘ negative, or â€” neutral (skip). Designed for speed â€” you can rate a full conversation in seconds.

### ğŸ“ˆ Analytics Dashboard

The Results page aggregates data across all your eval sessions with three analysis views:

**Overview** â€” Provider comparison with horizontal bar charts showing average TTFB and human ratings side-by-side. Instantly see which provider performs best.

**By Prompt** â€” Which scenarios each provider handles well or poorly. Sorted by usage count. Click a prompt to filter the session table.

**By Metric** â€” Per-metric distribution across all sessions. Color-coded: green (>70%), orange (40-70%), red (<40%). See exactly where each provider falls short.

Plus: CSV export of any filtered view, date range filtering (7d/30d/all), provider and status filters.

### ğŸ“ Prompts Library

75+ built-in evaluation scenarios organized by category:

- **Task Completion** â€” Restaurant booking, appointment scheduling, order placement, travel planning
- **Information Retrieval** â€” FAQ lookup, product details, weather queries, knowledge questions
- **Conversation Flow** â€” Multi-turn dialogue, context retention, topic switching, error recovery

Each scenario includes difficulty rating (easy/medium/hard), expected outcome, and tags. Create custom prompts via the UI or bulk import from YAML.

## ğŸ”Œ Providers

Extensible adapter architecture â€” add any voice AI provider:

| Provider | Type | Pipeline | Status |
|----------|------|----------|--------|
| **OpenAI Realtime** | `openai` | Whisper â†’ GPT-4o â†’ TTS | âœ… Built-in |
| **Google Gemini** | `gemini` | Gemini multimodal â†’ Cloud TTS | âœ… Built-in |
| **Retell AI** | `retell` | End-to-end voice agent API | âœ… Built-in |
| **ElevenLabs** | `elevenlabs` | Coming soon | ğŸ”œ |
| **Custom** | `custom` | Bring your own endpoint | âœ… Supported |

Configure providers through the Settings UI or programmatically:

```bash
# OpenAI Realtime
curl -X POST http://localhost:3000/api/providers \
  -H "Content-Type: application/json" \
  -d '{
    "name": "GPT-4o Realtime",
    "type": "openai",
    "config": { "apiKey": "sk-...", "model": "gpt-4o-realtime", "voiceId": "nova" }
  }'
```

### Writing a Custom Adapter

```typescript
import { ProviderAdapter } from '@/lib/providers/base-adapter';
import type { AudioPrompt, ProviderResponse } from '@/lib/providers/types';

export class MyAdapter extends ProviderAdapter {
  async generateResponse(prompt: AudioPrompt): Promise<ProviderResponse> {
    // Your voice provider logic here
    // Return: { audioBuffer, transcript, metadata }
  }

  async healthCheck() {
    // Return: { healthy: boolean, latencyMs: number }
  }
}
```

## ğŸ—ï¸ Architecture

```
voicebench/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ eval/         # Sessions, turns, ratings, analytics
â”‚   â”‚   â”‚   â”œâ”€â”€ providers/    # Provider CRUD + health checks
â”‚   â”‚   â”‚   â””â”€â”€ scenarios/    # Prompt management + YAML import
â”‚   â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”‚   â”œâ”€â”€ live/         # Live conversation + real-time metrics
â”‚   â”‚   â”‚   â””â”€â”€ demo/         # Demo with sample data
â”‚   â”‚   â”œâ”€â”€ results/          # Analytics dashboard + session detail
â”‚   â”‚   â”œâ”€â”€ prompts/          # Scenario library management
â”‚   â”‚   â””â”€â”€ settings/         # Provider configuration
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ layout/           # Sidebar navigation
â”‚   â”‚   â”œâ”€â”€ settings/         # Provider form + list
â”‚   â”‚   â””â”€â”€ ui/               # shadcn/ui components
â”‚   â”œâ”€â”€ db/                   # Drizzle ORM schemas (SQLite)
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ providers/        # Adapter system (OpenAI, Gemini, Retell)
â”‚   â”‚   â”œâ”€â”€ eval/             # WER calculator, metrics collector, LLM judge
â”‚   â”‚   â””â”€â”€ services/         # Business logic
â”‚   â””â”€â”€ types/                # TypeScript interfaces
â”œâ”€â”€ data/                     # SQLite database (gitignored)
â””â”€â”€ docs/                     # Screenshots, architecture docs
```

### Tech Stack

| Layer | Technology |
|-------|-----------|
| **Framework** | Next.js 16 (App Router, React 19) |
| **Language** | TypeScript 5 |
| **Database** | SQLite + Drizzle ORM |
| **UI** | shadcn/ui, Tailwind CSS 4 |
| **State** | Zustand |
| **Validation** | Zod |

## ğŸ“œ Scripts

```bash
npm run dev          # Development server (localhost:3000)
npm run build        # Production build
npm run db:push      # Push schema to database
npm run db:seed      # Seed sample data + demo prompts
npm run db:studio    # Open Drizzle Studio (DB browser)
```

## ğŸ¤ Contributing

Contributions welcome:

1. Fork the repo
2. Create a feature branch (`git checkout -b feat/my-feature`)
3. Make sure `npm run build` passes
4. Open a PR

Ideas for contributions:
- New provider adapters (ElevenLabs, PlayHT, Deepgram)
- Additional auto-detected metrics
- Batch evaluation mode (run N prompts sequentially)
- Real-time audio waveform visualization
- Team/workspace features

## ğŸ“„ License

MIT â€” see [LICENSE](./LICENSE) for details.

## ğŸ“¬ Contact

Questions, feedback, or want to collaborate? Reach out at **mhmdez@me.com**.

---

<p align="center">
  Built for teams shipping voice AI
</p>
