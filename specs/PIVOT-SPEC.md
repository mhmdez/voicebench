# VoiceBench v3 â€” Voice AI Evaluation Workbench

## What Is It?
A developer tool for evaluating voice AI agents. Connect your providers, run live conversations, get real-time metrics (auto-detected + human-rated), and analyze results across models, prompts, and sessions.

## Pages

### 1. Providers (`/settings`)
**Already exists.** Add API keys for voice AI providers.
- OpenAI, Gemini, Retell AI, ElevenLabs, custom
- Test connection button
- Keep as-is, minor cleanup

### 2. Prompts (`/prompts`)
**New page.** Manage test prompts/scenarios.
- List of prompts with category tags
- Create/edit/delete prompts
- Each prompt has: text, category, expected outcome (optional), difficulty
- Import/export prompts (JSON/CSV)
- 75 scenarios already seeded â€” show them here

### 3. Live Eval (`/eval/live`)
**The core feature.** Pick a prompt + provider â†’ start a live voice conversation.

#### Setup
- Select provider from dropdown
- Select prompt (or type freestyle)
- Hit "Start Session"

#### During Conversation
Split layout:
- **Left: Conversation** â€” real-time transcript with audio playback per turn
- **Right: Metrics Dashboard** â€” live-updating charts

**Auto-detected metrics (update automatically per turn):**
| Metric | How |
|--------|-----|
| Latency (TTFB) | Time from end of user speech to first agent audio byte |
| Total Response Time | Full round-trip |
| Turn-taking Latency | Gap between user stop â†’ agent start |
| Word Error Rate | Compare STT transcript to expected (if available) |
| Response Length | Word count per turn |
| Speech Rate | Words per minute of agent response |
| Interruption Recovery | If user interrupts, does agent stop and adapt? (detect via overlap) |

**Human-rated metrics (quick buttons per turn):**
| Metric | Input |
|--------|-------|
| Naturalness | ğŸ‘/ğŸ‘ â€” Does it sound human? |
| Prosody | ğŸ‘/ğŸ‘ â€” Appropriate intonation, rhythm, stress? |
| Emotion | ğŸ‘/ğŸ‘ â€” Appropriate emotional tone? |
| Accuracy | ğŸ‘/ğŸ‘ â€” Correct information? |
| Helpfulness | ğŸ‘/ğŸ‘ â€” Actually useful response? |
| Efficiency | ğŸ‘/ğŸ‘ â€” Concise, not rambling? |
| Voice Consistency | ğŸ‘/ğŸ‘ â€” Same character/voice throughout? |

Each turn shows a compact row of thumb buttons. One click per metric per turn. Fast.

#### End of Session
- Summary card with aggregate scores
- Auto-save to results database
- Option to add notes

### 4. Results (`/results`)
**Analysis page.** All eval sessions with comprehensive filtering.

#### Views
- **By Model** â€” Compare providers side-by-side across all metrics
- **By Prompt** â€” How does a specific prompt perform across models?
- **By Session** â€” Drill into individual eval sessions, turn-by-turn
- **By Metric** â€” Trend a specific metric across all sessions

#### Charts
- Bar charts for metric comparisons
- Radar chart for multi-metric provider comparison
- Time series for latency trends
- Distribution plots for score spreads

#### Export
- JSON, CSV export of all data
- Shareable session links

## Navigation
```
VoiceBench
  Eval (live eval page â€” the main feature)
  Results
  Prompts
  Settings (providers)
```

## What We Keep
- Provider management (Settings)
- Scenario/prompt database + seed data
- Drizzle ORM + SQLite
- shadcn/ui components
- Basic app shell

## What We Remove
- Arena (blind A/B polling)
- Leaderboard (Elo rankings)
- Old eval runs page
- Marketing homepage

## Tech Notes
- Real-time audio: WebSocket to provider API (OpenAI Realtime, Gemini Live)
- Transcript: Whisper or provider's built-in STT
- Charts: Recharts or Tremor (Tremor already installed)
- Human ratings stored per-turn in SQLite
- Auto-metrics computed server-side, pushed to client via SSE/WebSocket
